# word2vec study

---
layout: single
title:  "word2vec 공부"
---


**이 글은 밑바닥부터 시작하는 딥러닝 2 책을 참고로 작성**

[밑바닥부터 시작하는 딥러닝 2](https://github.com/WegraLee/deep-learning-from-scratch-2)

[참조 블로그](https://velog.io/@dscwinterstudy/2020-02-02-0002-%EC%9E%91%EC%84%B1%EB%90%A8-pwk63r14ez)

1.1 추론 기반 기법과 신경망
---

단어를 벡터로 표현하는 방법은 크게 두 부분이 있다.

1. 통계 기반 기법

2. 추론 기반 기법

두 기법의 배경에는 모두 분포 가설이 있다.


### 1.1.1 통계 기반 기법의 문제점

통계 기반 기법의 대표적인 모델은 SVD이다.

말뭉치 안의 각 단어에 대해서 예측할 단어의 주변 단어의 빈도를 집계(동시발생 행렬) 해서

동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 "희소벡터"를 작은

"밀집벡터"로 변환해 분산 표현을 얻는 방식이다.

하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.

SVD를 n*n 헹렬에 적용하는 비용은 O(n3) 이다.

한편, 추론 기반 기법에서는 미니배치로 학습하는 것이 일반적이다.

![1](/assets/images/word2vec/1.PNG)

통계 기반 기법은 학습 데이터를 한꺼번에 처리한다.

반면, 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습한다.


### 1.1.2 추론 기반 기법 개요

추론 기반 기법에서는 추론이 주된 작업이다.

추론이란, 주변 단어(맥락)이 주어졌을 때, ? 에 무슨 단어가 들어가는지를 추측하는 작업이다.

![2](/assets/images/word2vec/2.PNG)

추론 문제를 풀고 학습하는 것이 추론 기반 기법이 다루는 문제이다.

이러한 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습하는 것이다.

즉, 예측할 단어의 주변 단어(맥락)을 가지고 심층신경망을 통해 나온 확률 분포로 추론한다.

![3](/assets/images/word2vec/3.PNG)


### 1.1.3 신경망에서의 단어 처리

심경망에서 학습을 하려면 단어들을 벡터로 바꿔야 한다. 

이때 원핫 표현을 통해 단어를 벡터로 변환한다.

![4](/assets/images/word2vec/4.PNG)

단어를 벡터로 나타낼 수 있고, 신경망을 구성하는 계층들은 벡터를 처리할 수 있다.

**즉, 단어를 신경망으로 처리할 수 있다.**

원 핫표현으로 된 단어 하나를 완전연결계층을 통해 변환하는 모습

![5](/assets/images/word2vec/5.PNG)

맥락 c와 가중치 W의 곱으로 해당 위치의 행벡터가 추출된다.

![6](/assets/images/word2vec/6.PNG)

W의 열의 개수가 많으면 가중치를 계산하는 과정에서 속도가 느려진다.

따라서 최대한 작게 만들어 압축하는 것이 좋다.


1.2 단순한 word2vec
---

지금 부터 할 일은 모델을 신경망으로 구축하는 것이다.

word2vec에서 제안하는 CBOW, continuous bag-of-words모델을 구현한다.


### 1.2.1 CBOW 모델의 추론 처리

CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다.

타깃은 중앙 단어이고, 그 주변 단어들이 맥락이다.

우리는 이 CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낼 것이다.

![7](/assets/images/word2vec/7.PNG)

위 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문이다.

은닉층을 보면 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환되는 데, 전체를 평균한 값이 은닉층으로 간다.

출력층은 각 단어들의 score값으로 각 Softmax 계층에 통과하면 가장 높은 값이 해당 단어를 출력할 확률도 높아진다. 

#### 이제 CBOW 모델을 계층 관점에서 본다.

![8](/assets/images/word2vec/8.PNG)

다음과 같이 예측할 단어(say)의 맥락(그림에서는 you, goodbye)을 원핫벡터로 나타내고, 

가중치 행렬(7x3)에 곱한 후 더해준다. 또한 단어의 개수만큼 나눠준 후 다시한번 가중치를 곱해 score를 도출한다.


### 1.2.2 CBOW 모델의 학습

지금까지 설명한 CBOW 모델은 출력층에서 각 단어의 score값을 출력했다.

이제 score 값을 softmax 함수에 적용하면 확률을 얻을 수 있다.

![9](/assets/images/word2vec/9.PNG)

CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정한다.

그 결과, 입/출력측 가중치에 단어의 출현 패턴을 파악한 벡터가 학습된다.

#### 이제 도출된 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후 
#### 그 값을 손실로 사용해 학습을 진행한다.

![10](/assets/images/word2vec/10.PNG)

추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy 계층을 추가한 

것만으로도 손실을 얻을 수 있다.


### 1.2.3 word2vec 의 가중치와 분산 표현

word2vec 에서 사용되는 신경망에는 두 가지 가중치가 있다.

입력 측 가중치와 출력 측 가중치이다.

그리고 입력 측 가중치의 각 행이 각 단어의 분산 표현에 해당한다.

다만, 출력 측 가중치는 각 단어의 분산 표현이 열 방향으로 저장된다.

![11](/assets/images/word2vec/11.PNG)

####  최종적으로 이용하는 단어의 분산 표현으로는 어느 쪽 가중치를 사용하면 좋을까?

1. 입력 측의 가중치만 이용

2. 출력 측의 가중치만 이용

3. 양쪽 가중치를 모두 이용

**word2vec, 특히 skip-gram 모델에서는 입력 측 가중치만 이용하는 것이 가장 대중적이다.**


1.3 학습 데이터 준비
---

### 1.3.1 맥락과 타깃

word2vec에서 이용하는 신경망의 입력은 맥락이다.

그리고 정답 레이블은 맥락에 둘러싸인 중앙의 단어, 즉 타깃이다.

우리가 해야 할 일은 신경망에 맥락을 입력했을 때 타깃이 출현할 확률을 높이는 것이다.

예를 들면,

![12](/assets/images/word2vec/12.PNG)

말뭉치로부터 맥락과 타깃을 만드는 함수를 구현하기 전에,

말뭉치 텍스트를 단어 ID 로 변환해야 한다.

```python
import sys
sys.path.append('..')
from common.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
print(corpus)
# [0 1 2 3 4 1 5 6

print(id_to_word)
# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'} 
```

단어 ID의 배열로부터 맥락과 타깃을 작성하는 예 (맥락의 윈도우 크기는 1)

![13](/assets/images/word2vec/13.PNG)

맥락과 타깃을 만드는 함수를 구현하면

```python
def create_co_matrix(corpus, vocab_size, window_size=1):
    '''동시발생 행렬 생성
    :param corpus: 말뭉치(단어 ID 목록)
    :param vocab_size: 어휘 수
    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)
    :return: 동시발생 행렬
    '''
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
            left_idx = idx - i
            right_idx = idx + i

            if left_idx >= 0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] += 1

            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1

    return co_matrix
```
다음과 같이 구현했다. 이제 맥락과 타깃의 원소를 원핫 표현으로 변환 한 후

CBOW 모델에 넘겨주면 된다.


### 1.3.2 원핫 표현으로 변환

지금 까지 말뭉치들을 CBOW 모델에 학습하기 위한 벡터로 변환 해주었다.

![14](/assets/images/word2vec/14.PNG)

그림과 같이 말뭉치들을 맥락(입력)과 타깃(출력)으로 나눠준 후

단어 ID를 통해 숫자로 변환 해주고, 

원핫 표현으로 0과 1의 표현으로 학습할 준비가 끝났다.

1.4 CBOW 모델 구현
---

CBOW 모델을 구현 해보자.

![15](/assets/images/word2vec/15.PNG)

```python
import sys
sys.path.append('..')
import numpy as np
from common.layers import MatMul, SoftmaxWithLoss
```

```python
class SimpleCBOW:
    def __init__(self, vocab_size, hidden_size):
        V, H = vocab_size, hidden_size

        # 가중치 초기화
        W_in = 0.01 * np.random.randn(V, H).astype('f')
        W_out = 0.01 * np.random.randn(H, V).astype('f')

        # 계층 생성
        self.in_layer0 = MatMul(W_in)
        self.in_layer1 = MatMul(W_in)
        self.out_layer = MatMul(W_out)
        self.loss_layer = SoftmaxWithLoss()

        # 모든 가중치와 기울기를 리스트에 모은다.
        layers = [self.in_layer0, self.in_layer1, self.out_layer]
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

        # 인스턴스 변수에 단어의 분산 표현을 저장한다.
        self.word_vecs = W_in
```

신경망의 순전파인 forward() 메서드를 구현한다.

```python
    def forward(self, contexts, target):
        h0 = self.in_layer0.forward(contexts[:, 0])
        h1 = self.in_layer1.forward(contexts[:, 1])
        h = (h0 + h1) * 0.5
        score = self.out_layer.forward(h)
        loss = self.loss_layer.forward(score, target)
        return loss
```

마지막으로 역전파인 backward()를 구현한다.

```python
    def backward(self, dout=1):
        ds = self.loss_layer.backward(dout)
        da = self.out_layer.backward(ds)
        da *= 0.5
        self.in_layer1.backward(da)
        self.in_layer0.backward(da)
        return None
```

CBOW 모델의 역전파

![16](/assets/images/word2vec/16.PNG)

### 1.4.1 학습 코드 구현

CBOW 모델의 학습은 일반적인 신경망의 학습과 완전히 같다.

맥락과 타깃이 각각 입력값과 출력값이 되고, 학습 데이터를 신경망에 입력한 다음,

모델에 나온 스코어값(score)에 softmax 함수를 이용해 확률 표현을 만든 다음,

실제값과 예측값을 비교 및 계산을 한다.

마지막으로 Loss(기울기)를 구하고 가중치 매개변수를 순서대로 갱신해간다.

```python
import sys
sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
from common.trainer import Trainer
from common.optimizer import Adam
from simple_cbow import SimpleCBOW
from common.util import preprocess, create_contexts_target, convert_one_hot


window_size = 1
hidden_size = 5
batch_size = 3
max_epoch = 1000

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)
contexts, target = create_contexts_target(corpus, window_size)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)

model = SimpleCBOW(vocab_size, hidden_size)
optimizer = Adam()
trainer = Trainer(model, optimizer)

trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()
```

매개변수 갱신 방법은 Adam으로 한다.

코드 결과

![17](/assets/images/word2vec/17.PNG)

다음과 같은 함수가 출력 됐다. 

학습을 거듭할수록 손실이 줄어드는 것을 확인할 수 있다.

1.5 word2vec 보충
---

지금까지 word2vec의 CBOW 모델을 살펴봤다.

이제 CBOW 모델을 확률 관점에서 살펴보자.


### 1.5.2 CBOW 모델과 확률

확률의 표기법을 간단하게 살펴보자.

확률 P()

동시 확률 P(A,B), A와 B가 동시에 일어날 확률.

사후 확률 P(AㅣB), 사건이 일어난 후의 확률.

B라는 정보가 주어졌을 때, A가 일어날 확률.<br />

그럼 CBOW 모델을 확률 표기법으로 기술해보자.

CBOW 모델이 하는 일은, 맥락을 주면 타깃 단어가 출현할 확률을 출력하는 것이다.

![18](/assets/images/word2vec/18.PNG)

맥락의 단어로부터 타깃 단어를 추측하는 것이다.

CBOW 모델은 다음 식을 모델링 하고 있다.

![19](/assets/images/word2vec/19.PNG)

위의 식을 이용하면 CBOW 모델의 손실 함수도 간결하게 표현할 수 있다.
 
교차 엔트로피 오차에 적용하여 나타내면, 
 
![20](/assets/images/word2vec/20.PNG)


### 1.5.3 skip-gram 모델

word2vec은 2개의 모델을 제안하고 있다.
1. CBOW 모델
2. skip-gram 모델<br />

skip-gram은 CBOW 에서 다루는 맥락과 타깃을 역전시킨 모델이다.

![21](/assets/images/word2vec/21.PNG)

다음과 같이 skip-gram 모델은 중앙의 단어(타깃)으로 부터 맥락을 추측한다.

![22](/assets/images/word2vec/22.PNG)

skip-gram 모델을 확률 표기로 나타내보면,

![23](/assets/images/word2vec/23.PNG)

위 식을 교차 엔트로피 오차에 적용하여 skip-gram 모델의 손실 함수를 유도할 수 있다.

![24](/assets/images/word2vec/24.PNG)

<br />

그러면 CBOW 모델과 skip-gram 모델 중 어느 것을 사용해야 할까?

정답은 skip-gram 모델이다.

단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많기 때문이다.

특히 말뭉치가 커질수록 유추 문제의 성능 면에서 skip-gram 모델이 더 뛰어나다.
