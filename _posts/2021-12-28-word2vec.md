# word2vec study

---
layout: single
title:  "word2vec 공부"
---


**이 글은 밑바닥부터 시작하는 딥러닝 2 책을 토대로 작성**

[밑바닥부터 시작하는 딥러닝 2](https://github.com/WegraLee/deep-learning-from-scratch-2)

1.1 추론 기반 기법과 신경망
---

단어를 벡터로 표현하는 방법은 크게 두 부분이 있다.

1. 통계 기반 기법

2. 추론 기반 기법

두 기법의 배경에는 모두 분포 가설이 있다.


### 1.1.1 통계 기반 기법의 문제점

통계 기반 기법의 대표적인 모델은 SVD이다.

말뭉치 안의 각 단어에 대해서 예측할 단어의 주변 단어의 빈도를 집계(동시발생 행렬) 해서

동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 "희소벡터"를 작은

"밀집벡터"로 변환해 분산 표현을 얻는 방식이다.

하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.

SVD를 n*n 헹렬에 적용하는 비용은 O(n3) 이다.

한편, 추론 기반 기법에서는 미니배치로 학습하는 것이 일반적이다.

![1](/assets/images/word2vec/1.PNG)

통계 기반 기법은 학습 데이터를 한꺼번에 처리한다.

반면, 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습한다.


### 1.1.2 추론 기반 기법 개요

추론 기반 기법에서는 추론이 주된 작업이다.

추론이란, 주변 단어(맥락)이 주어졌을 때, ? 에 무슨 단어가 들어가는지를 추측하는 작업이다.

![2](/assets/images/word2vec/2.PNG)

추론 문제를 풀고 학습하는 것이 추론 기반 기법이 다루는 문제이다.

이러한 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습하는 것이다.

즉, 예측할 단어의 주변 단어(맥락)을 가지고 심층신경망을 통해 나온 확률 분포로 추론한다.

![3](/assets/images/word2vec/3.PNG)


### 1.1.3 신경망에서의 단어 처리

심경망에서 학습을 하려면 단어들을 벡터로 바꿔야 한다. 

이때 원핫 표현을 통해 단어를 벡터로 변환한다.

![4](/assets/images/word2vec/4.PNG)

단어를 벡터로 나타낼 수 있고, 신경망을 구성하는 계층들은 벡터를 처리할 수 있다.

**즉, 단어를 신경망으로 처리할 수 있다.**

원 핫표현으로 된 단어 하나를 완전연결계층을 통해 변환하는 모습

![5](/assets/images/word2vec/5.PNG)

맥락 c와 가중치 W의 곱으로 해당 위치의 행벡터가 추출된다.

![6](/assets/images/word2vec/6.PNG)

W의 열의 개수가 많으면 가중치를 계산하는 과정에서 속도가 느려진다.

따라서 최대한 작게 만들어 압축하는 것이 좋다.


1.2 단순한 word2vec
---

지금 부터 할 일은 모델을 신경망으로 구축하는 것이다.

word2vec에서 제안하는 CBOW, continuous bag-of-words모델을 구현한다.


### 1.2.1 CBOW 모델의 추론 처리

CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다.

타깃은 중앙 단어이고, 그 주변 단어들이 맥락이다.

우리는 이 CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낼 것이다.

![7](/assets/images/word2vec/7.PNG)

위 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문이다.

은닉층을 보면 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환되는 데, 전체를 평균한 값이 은닉층으로 간다.

출력층은 각 단어들의 score값으로 각 Softmax 계층에 통과하면 가장 높은 값이 해당 단어를 출력할 확률도 높아진다. 

#### 이제 CBOW 모델을 계층 관점에서 본다.

![8](/assets/images/word2vec/8.PNG)

다음과 같이 예측할 단어(say)의 맥락(그림에서는 you, goodbye)을 원핫벡터로 나타내고, 

가중치 행렬(7x3)에 곱한 후 더해준다. 또한 단어의 개수만큼 나눠준 후 다시한번 가중치를 곱해 score를 도출한다.


### 1.2.2 CBOW 모델의 학습

지금까지 설명한 CBOW 모델은 출력층에서 각 단어의 score값을 출력했다.

이제 score 값을 softmax 함수에 적용하면 확률을 얻을 수 있다.

![9](/assets/images/word2vec/9.PNG)

CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정한다.

그 결과, 입/출력측 가중치에 단어의 출현 패턴을 파악한 벡터가 학습된다.

#### 이제 도출된 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후 
#### 그 값을 손실로 사용해 학습을 진행한다.

![10](/assets/images/word2vec/10.PNG)

추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy 계층을 추가한 

것만으로도 손실을 얻을 수 있다.


### 1.2.3 word2vec 의 가중치와 분산 표현

word2vec 에서 사용되는 신경망에는 두 가지 가중치가 있다.

입력 측 가중치와 출력 측 가중치이다.

그리고 입력 측 가중치의 각 행이 각 단어의 분산 표현에 해당한다.

다만, 출력 측 가중치는 각 단어의 분산 표현이 열 방향으로 저장된다.

![11](/assets/images/word2vec/11.PNG)

####  최종적으로 이용하는 단어의 분산 표현으로는 어느 쪽 가중치를 사용하면 좋을까?

1. 입력 측의 가중치만 이용

2. 출력 측의 가중치만 이용

3. 양쪽 가중치를 모두 이용

**word2vec, 특히 skip-gram 모델에서는 입력 측 가중치만 이용하는 것이 가장 대중적이다.**


