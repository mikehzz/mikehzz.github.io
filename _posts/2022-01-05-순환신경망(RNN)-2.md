---
layout: single
title:  "[딥러닝 2-4]순환 신경망(Recurrent Neural Network)-2"

categories:
  - 딥러닝 이론
tags:
  - RNN
  - 기울기 소실
  - 기울기 폭발
  - 기울기 클리핑
---

**이 글은 밑바닥부터 시작하는 딥러닝 2 책을 참고로 작성**

[밑바닥부터 시작하는 딥러닝 2](https://github.com/WegraLee/deep-learning-from-scratch-2)

5.1 RNN의 문제점
---

RNN은 시계열 데이터의 장기 의존 관계를 학습하기 어렵다. 바로 BPTT에서의 기울기 소실 

혹은 기울기 폭발이 일어나기 때문이다.

### 5.1.1 RNN 복습

RNN 계층은 순환 경로를 갖고 있다. 이 순환 경로를 펼치면 그림과 같다.

![1](/assets/images/RNN_4/1.PNG)

위의 그림에서 보듯, RNN 계층은 시계열 데이터인 xt를 입력하면 ht를 출력한다.  
hi는 RNN계층의 은닉 상태라 하여 과거 정보를 저장한다.

RNN의 특징은 바로 이전 시각의 은닉 상태를 이용한다. 이렇게 해서 과거 정보를 계승할 수 있게 되고,  
RNN 계층이 수행하는 처리를 계산 그래프로 나타내면 밑의 그림처럼 나타난다.

![2](/assets/images/RNN_4/2.PNG)

RNN 계층의 순전파에서 수행하는 계산은 행렬의 곱과 합, 그리고 활성화 함수인 tanh 함수에  
의한 변환으로 구성된다.

그 다음으로 RNN 계층이 안고 있는 문제점, 즉 장기 기억에 취약하다는 문제를 살펴보자.

### 5.1.2 기울기 소실 또는 기울기 폭발

다음과 같은 문장에서 그 다음으로 올 단어인 [?] 를 유추해 보자. (RNNLM의 문제점)

#### Tom was watching TV in his room. Mary came into the room. Mary said hi to [?]

다음 [?]에 들어가는 단어는 "Tom"이다. RNNLM 모델에서는 [?]에 들어갈 단어를 예측하려면  
맨 앞의 단어인 "Tom" 이라는 단어를 기억해둬야 한다. 다시 말하면 이런 정보를 RNN 계층의 은닉 상태에  
인코딩해 보관해야 한다.

그러면 이 예를 RNNLM 학습의 관점에서 생각해보자.

![3](/assets/images/RNN_4/3.PNG)

RNNLM의 관점에서 정답 레이블이 'Tom'임을 학습할 때 중요한 것이 바로 RNN 계층의 존재이다.  
RNN 계층이 과거 방향으로 '의미 있는 기울기'를 전달함으로 시간 방향의 의존 관계를 학습할 수 있다.

여기서 기울기는 학습해야 할 의미가 있는 정보가 들어 있고, 그것을 과거로 전달함으로써 장기 의존 관계를  
학습한다. 하지만 중간에 기울기가 사그라들면 가중치 매개변수는 전혀 갱신되지 않게 되어 장기 의존 관계를  
학습할 수 없다. 현재는 RNN 계층에서 시간을 거슬러 올라가 기울기가 작아지거나 커져 소실 혹은 폭발된다.

### 5.1.3 기울기 소실과 기울기 폭발의 원인

![4](/assets/images/RNN_4/4.PNG)

위의 RNN 계층의 그림에서 시간 방향 기울기 전파에만 주목하자.  
길이가 T인 시계열 데이터를 가정하여 T번째 정답 레이블로부터 전해지는 기울기가 어떻게 변하는지 보면,  
앞의 문제에 대입하면 T번째 정답 레이블이 'Tom'인 경우에 해당한다. 이때 시간 방향 기울기에 주목하면 역전파로  
전해지는 기울기는 차례로 'tanh','+','MatMul(행렬곱)' 연산을 통과한다는 것을 알 수 있다.

'+'의 역전파는 상류에서 전해지는 기울기를 그대로 하류로 흘러보내 기울기는 변하지 않는다.

'tanh'의 경우와 미분된 경우를 그래프로 그리면 아래와 같다.

![5](/assets/images/RNN_4/5.PNG)

그림에서 점선이 y=tanh(x)의 미분이고 값은 1.0 이하이며, x가 0으로부터 멀어질수록 작아진다.  
즉, 역전파에서 기울기가 tanh 노드를 지날 때마다 값은 계속 작아진다는 의미이다. 그리고 tanh 함수를  
T번 통과하면 기울기도 T번 반복해서 작아진다.

다음은 RNN 계층의 행렬 곱에만 주목했을 때의 역전파의 기울기이다.

![6](/assets/images/RNN_4/6.PNG)

상류로부터 dh라는 기울기가 흘러나온다고 가정하고 이때 Matmul 노드에서의 역전파는 dh*Wh^T 라는 행렬  
곱으로 기울기를 계산한다. 그리고 같은 계산을 시계열 데이터의 시간 크기만큼 반복한다.

만약 Wh가 1보다 크면,

![7](/assets/images/RNN_4/7.PNG)

- 기울기의 크기는 시간에 비례해 지수적으로 증가한다.
- 이러한 현상은 **기울기 폭발(exploding gradients)**이다.
- 기울기 폭발이 일어나면 오버플로를 일으켜 NAN 같은 값을 발생시킨다.

만약 Wh가 1보다 작으면,

![8](/assets/images/RNN_4/8.PNG)

- 기울기가 지수적으로 감소한다.
- 이러한 현상은 **기울기 소실(vanishing gradients)이다.
- 기울기가 소실이 일어나면 기울기가 매우 빠르게 작아진다.
- 기울기가 일정 수준 이하로 작아지면 가중치 매개변수가 더 이상 갱신되지 않으므로, 장기 의존 관계를
학습할 수 없게된다.

### 5.1.4 기울기 폭발대책

기울기 폭발 대책으로는 전통적인 기법인 **기울기 클리핑**이라는 기법이 있다.

![8](/assets/images/RNN_4/8.PNG)

기울기 클리핑은 단순하며 여기서 신경망에서 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다고  
가정한다.  
기울기의 L2노름이 문턱값을 초과하면 두 번째 줄의 수식과 같이 기울기를 수정하며 이를 기울기 클리핑이라고 한다.

5.2 정리
---

- RNN의 문제점으로는 기울기 폭발과 기울기 소실이 있다.
- 두 문제 전부 Matmul의 미분 계산 과정(기울기)에서 발생하며 장기 의존성이 떨어진다.
- 기울기 폭발의 문제점을 극복하기 위해 기울기 클리핑이라는 기법을 사용한다.
- 기울기 소실의 대책은 다음 장에 알아본다.
