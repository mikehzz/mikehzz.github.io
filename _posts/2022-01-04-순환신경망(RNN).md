---
layout: single
title:  "순환신경망(Recurrent Neural Network)"

categories:
  - 딥러닝 공부
tags:
  - RNN
  - BPTT
  - RNNLM
  - Time RNN
  - Truncated BPTT
---

**이 글은 밑바닥부터 시작하는 딥러닝 2 책을 참고로 작성**

[밑바닥부터 시작하는 딥러닝 2](https://github.com/WegraLee/deep-learning-from-scratch-2)

1.1 RNN
---

### 1.1.1 RNN의 배경

word2vec에서 배운 CBOW 모델은 타깃이 '중앙 단어'이고, 맥락은 '주변 단어'이었다.

![1](/assets/images/RNN_3/1.PNG)

즉, 맥락을 좌우 대칭으로 생각한다

t번째 단어를 타깃으로 그 전후 단어(t-1)번째 단어와 (t+1)번째 단어를 **맥락**으로 취급

이번에는 맥락을 왼쪽 윈도우 만으로 한정하자.

![2](/assets/images/RNN_3/2.PNG)

w(t-2)과 w(t-1)이 주어졌을 때 타깃이 wt가 될 확률(CBOW 모델이 출력할 확률)을 수식으로 표현하면

![3](/assets/images/RNN_3/3.PNG)

CBOW 모델이 다루는 손실함수 (교차엔트로피 오차)

![4](/assets/images/RNN_3/4.PNG)

CBOW 모델을 학습시키는 본래 목적은 맥락으로부터 타깃을 정확하게 추측하는 것이다.

그러면 '맥락으로부터 타깃을 추측하는 것'은 어디에 이용할 수 있을까?

언어 모델에 등장한다.

언어 모델은 단어 나열에 확률을 부여한다.

특정한 단어의 시퀀스에 대해서 그 시퀀스가 일어날 가능성이 어느 정도인지를 확률로 평가한다.

![5](/assets/images/RNN_3/5.PNG)

- word2vec의 CBOW 모델을 언어 모델에 적용하려면 맥락의 크기를 특정 값으로 한정하여   
근사적으로 나타낼 수 있다.
- 맥락의 크기는 임의 길이로 설정할 수 있지만 결국 특정 길이로 '고정'된다.
- CBOW 모델의 맥락 크기를 키울 수 있으나 맥락 안의 단어 순서가 무시된다는 한계가 있다.
- 맥락의 단어 순서를 고려하기 위해 맥락의 단어 벡터를 은닉층에서 연결하는 방식을 생각할 수 있으나  
맥락의 크기에 비례해 가중치 매개 변수가 늘어난다는 문제가 발생한다. 

따라서 순환 신경망 즉, **RNN**이 등장하게 되었다. 

RNN은 맥락이 아무리 길더라도 맥락의 정보를 기억하는 메커니즘을 갖추고 있어 긴 시계열 데이터에도 대응할 수 있다.

### 1.1.2 순환하는 신경망

순환하기 위해서는 **닫힌 경로**가 필요하다.

**닫힌 경로** 혹은 **순환하는 경로**가 존재해야 데이터가 같은 장소를 반복해 왕래할 수 있고

데이터가 순환하면서 과거의 정보를 기억하는 동시에 최신 데이터로 갱신 될 수 있다.

![6](/assets/images/RNN_3/6.PNG)

xt = (x0, x1, ..., xt)가 RNN계층에 입력되고 이에 대응해 (h0,h1, ...,ht)가 출력된다.

각 시각에 입력되는 xt를 벡터라고 가정했을 때

문장(단어 순서)을 다루는 경우를 예로 든다면 각 단어의 분산 표현이 xt가 되며 이 분산 표현이

순서대로 하나씩 RNN계층에 입력된다.

### 1.1.3 순환구조 펼치기

![7](/assets/images/RNN_3/7.PNG)

위 그림에서는 다수의 RNN계층 모두가 실제로는 '같은 계층'인 것이 지금까지의 신경망과는 다른 점이다.

각 시각의 RNN계층은 그 계층으로의 입력과 1개 전의 RNN계층으로부터의 출력을 받는데 이 두 정보를

바탕으로 현 시각의 출력을 계산한다.

![8](/assets/images/RNN_3/8.PNG)
 
Wx : 입력 x를 출력 h로 변환하기 위한 가중치

Wh : 1개의 RNN출력을 다음 시각의 출력으로 변환하기 위한 가중치

b : 편향

h(t-1), xt : 행벡터

ht는 다른 계층을 향해 위쪽으로 출력되는 동시에 다음 시각의 RNN계층을 향해  
오른쪽으로도 출력된다. RNN의 출력 ht는 은닉상태라고 한다.

RNN은 h라는 '상태'를 가지고 있으며 위의 식의 형태로 갱신된다고 해석할 수 있다.

1.2 BPTT
---

![9](/assets/images/RNN_3/9.PNG)

순환 구조를 펼친 후의 RNN에는 오차역전파법을 적용할 수 있다. 

먼저 순전파를 수행하고 이어서 역전파를 수행하여 원하는 기울기를 구할 수 있다.

여기서의 오차역전파법은 '시간 방향으로 펼친 신경망의 오차역전파법'이란 뜻으로

BPTT(Backpropagation Through Time)라고 한다.

#### 문제점

- 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가
- 시간 크기가 커지면 역전파 시의 기울기가 불안정해짐

1.3 Truncated BPTT
---

BPTT의 해결법으로 적당 지점에서 끊어 학습을 하는 방법이다.

시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라내어 작은 신경망 여러 개로

만들어 잘라낸 작은 신경망에서 오차역전파법을 수행한다.

![10(/assets/images/RNN_3/10.PNG)

- 적당 지점에서 끊어 학습
- 각각의 블록 단위로, 미래의 블록과는 독립적인 오차역전파 완결
- 역전파의 연결은 끊어짐
- 순전파의 연결은 끊어지지 않음

1.4 RNN 계층 구현
---

![8](/assets/images/RNN_3/8.PNG)

행렬 곱에서는 대응하는 차원의 원소 수를 일치시킨다.

![11(/assets/images/RNN_3/11.PNG)

N : 미니배치 크기, D : 입력 벡터의 차원 수, H : 은닉 상태 벡터의 차원 수

#### 처리를 한 단계만 수행하는 RNN클래스 구현

```python
class RNN:
    def __init__(self, Wx ,Wh ,b): #가중치 2개, 편향 1개
        self.params = [Wx, Wh, b] # 인수로 매개변수를 인스턴스 변수 params에 리스트로 저장
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None
        
    def forward(self, x, h_prev): # 아래로부터의 입력 x, 왼쪽으로부터의 입력 h_prev
        Wx, Wh, b = self.params
        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b  # RNN 계산식
        h_next = np.tanh(t)
        
        self.cache = (x,h_prev, h_next) # h_prev : 이전 RNN계층으로의 입력, h_next : 현 RNN 계층으로부터의 출력
        return h_next
        
    def backward(self, dh_next):
        Wx, Wh, b = self.params
        x, h_prev, h_next = self.cache

        dt = dh_next * (1 - h_next ** 2)
        db = np.sum(dt, axis=0)
        dWh = np.matmul(h_prev.T, dt)
        dh_prev = np.matmul(dt, Wh.T)
        dWx = np.matmul(x.T, dt)
        dx = np.matmul(dt, Wx.T)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        return dx, dh_prev
```

1.5 Time RNN 계층 구현
---

RNN계층 T개를 연결한 신경망 Time RNN 계층

```python
#RNN클래스를 이용해 T개 단계의 처리를 한꺼번에 수행하는 계층을 
#TimeRNN이란 이름의 클래스로 완성한다.
class TimeRNN:
  def __init__(self, Wx, Wh, b, stateful=false):
    #초기화 메서드는 가중치, 편향, stateful이라는 boolean값을 인수로 받음
    #stateful=True : Time RNN계층이 은닉 상태를 유지한다.->아무리 긴 시계열 데이터라도 Time RNN계층의 순전파를 끊지 않고 전파한다.
    #stateful=False: Time RNN 계층은 은닉 상태를 '영행렬'로 초기화한다.상태가 없다.
    self.params = [Wx, Wh, b]
    self.grads = [np.zeors_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
    self.layers = None
    #layers : 다수의 RNN계층을 리스트로 저장하는 용도
    
    self.h, self.dh = None, None
    #h: forward() 메서드를 불렀을 때 마지막 RNN 계층의 은닉 상태를 저장
    #dh: backward()를 불렀을 때 하나 앞 블록의 은닉 상태의 기울기를 저장한다.
    self.stateful = stateful
    
  def set_state(self, h):
    #Time RNN계층의 은닉 상태를 설정하는 메서드
    self.h = h
    
  def reset_state(self):
    #은닉 상태를 초기화하는 메서드
    self.h = None

#순전파 구현
def forward(self, xs):
  #아래로부터 입력 xs(T개 분량의 시계열 데이터를 하나로 모은 것)를 받는다.
  Wx, Wh, b = self.params
  N, T, D = xs.shape #N: 미니배치 크기 D: 입력 벡터의 차원 수
  D, H = Wx.shape
  
  self.layers = []
  hs = np.empty((N, T, H), dtype='f')
  #출력값을 담을 그릇 hs를 준비한다.
  
  if not self.stateful or self.h is None:
    self.h = np.zeros((N, H), dtype='f')
    #h: RNN 계층의 은닉 상태. 
    #self.h=None: 처음 호출 시에는 원소가 모두 0인 영행렬로 초기화됨.
    #stateful=False: 항상 영행렬로 초기화
    
  for t in range(T):
    layer = RNN(*self.params)
    # *: 리스트의 원소들을 추출하여 메서드의 인수로 전달
    #self.params에 들어 있는 Wx, Wh, b를 추출하여 RNN 클래스의 __init__()메서드에 전달
    #RNN계층을 생성하여 인스턴스 변수 layers에 추가한다.
    self.h = layer.forward(xs[:, t, :], self.h)
    hs[:, t, :] = self.h
    self.layers.append(layer)
    
  return hs

#역전파 구현
def backward(self, dhs):
  Wx, Wh, b = self.params
  N, T, H = dhs.shape
  D, H = Wx.shape
  
  dxs = np.empty((N, T, D), dtype='f')
  dh = 0
  grads = [0, 0, 0]
  for t in reversed(range(T)):
    layer = self.layers[t]
    dx, dh = layer.backward(dhs[:, t, :] + dh) #합산된 기울기
    #RNN계층의 순전파에서는 출력이 2개로 분기되는데 역전파에서는 
    #각 기울기가 합산되어 전해진다.
    dxs[:, t, :] = dx
    
    for i, grad in enumerate(layer.grads):
      grads[i] +=grad
      
  for i, grad in enumerate(grads):
    self.grads[i][...] = grad
  self.dh = dh
  
  return dxs
```
![12(/assets/images/RNN_3/12.PNG)

- (상류) 출력 층에서는 전해지는 기울기 dhs 사용
- (하류) 입력 층으로 내보내는 기울기 dxs로 사용

![13(/assets/images/RNN_3/13.PNG)

Time 계층

- Time Embedding - 순전파 시 T개의 Embedding 계층 준비, 각 계층이 각 시각의 데이터 처리
- Time Affine 계층 - Affine 게층을 T개 준비하여 각 시각의 데이터를 개별 처리

![14(/assets/images/RNN_3/14.PNG)

최종 Loss - T개의 Softmax with Loss 계층 각각의 손실을 산출하여 합산해 평균한 값

![15(/assets/images/RNN_3/15.PNG)

1.6 RNNLM (RNN Language Model) 구현
---

#### RNN을 사용한 언어 모델

![16(/assets/images/RNN_3/16.PNG)

Embedding : 단어 ID를 단어의 분산 표현(단어 벡터)으로 변환

RNN계층 : 은닉 상태를 다음 층으로(위쪽으로) 출력함과 동시에 다음 시각의  
RNN 계층으로(오른쪽으로) 출력한다.  
RNN계층이 위로 출력한 은닉 상태는 Affine 계층을 거쳐 Softmax 계층으로 전해진다.

RNNLM은 지금까지 입력된 단어를 '기억'하고 그것을 바탕으로 다음에 출현할 단어를 예측한다.  
RNN계층이 과거에서 현재로 데이터를 계속 흘려보내줌으로써 과거의 정보를 인코딩해 저장 할 수 있다.

1.7 정리
---

- RNN은 순환하는 경로가 있고, 이를 통해 내부에 '은닉 상태'를 기억할 수 있다.
- RNN의 순환 경로를 펼침으로써 다수의 RNN 계층이 연결된 신경망으로 해석할 수 있으며,  
보통의 오차역전파법으로 학습할 수 있다(BPTT)
- 긴 시계열 데이터를 학습할 때는 데이터를 적당한 길이씩 모으고, 블록 단위로  
BPTT에 의한 학습을 수행한다(Truncated BPTT)
- Truncated BPTT에서는 역전파의 연결만 끊는다.
- Truncated BPTT에서는 순전파의 연결을 유지하기 위해 데이터를 '순차적'으로 입력해야 한다.
