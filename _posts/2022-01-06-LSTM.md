---
layout: single
title:  "[딥러닝 2-5]Long Short-Term Memory(LSTM)"

categories:
  - 딥러닝 이론
tags:
  - LSTM
  - forget gate
  - input gate
  - cell state
  - output gate
---

**이 글은 밑바닥부터 시작하는 딥러닝 2 책을 참고로 작성**

[밑바닥부터 시작하는 딥러닝 2](https://github.com/WegraLee/deep-learning-from-scratch-2)

6.1 기울기 소실과 LSTM
---

RNN 학습에서는 기울기 소실도 큰 문제이다.

여기서 등장하는 것이, 이번 장의 핵심 주제인 "게이트가 추가된 RNN" 이다.

그 대표격으로 LSTM, GRU가 있다.

### 6.1.1 LSTM의 인터페이스

RNN 계층과 LSTM 계층의 비교

![1](/assets/images/LSTM/1.PNG)

이 그림에서는 LSTM 계층의 인터페이스에는 c라는 경로가 있다는 차이가 있다.  
여기서 c를 **기억셀** 이라 하며 LSTM 전용의 기억 메커니즘이다.

기억셀의 특징은 LSTM 계층 내에서만 완결되고, 다른 계층으로는 출력하지 않는다.  
반면, LSTM의 은닉상태 h는 다른 계층으로 출력된다.

### 6.1.2 LSTM 계층 조립하기

LSTM에는 기억 셀 ct가 있다. ct에서 시각 t에서의 LSTM의 기억이 저장돼 있는데, 과거로부터  
시각 t까지의 필요한 모든 정보가 저장돼 있다고 가정한다.(실제로 그렇게 학습)  
그리고 필요한 정보를 모두 간직한 이 기억을 바탕으로, 외부 계층에 은닉 상태 ht를 출력한다.  
이때 ht는 다음과 같이 기얼 셀의 값을 tanh 함수로 변환한 값이다.

![2](/assets/images/LSTM/2.PNG)

그림과 같이 ct는 어떠한 계산을 수행해 구할 수 있고, 핵심은 ct를 사용해 ht를 계산한다는 것이다.  
여기서의 ct는 메모리라고 생각하면 편하다.  
ct는 cell gate라는 게이트에서 동작을 한다.

- 게이트

게이트는 "문"을 의미하는 단어이다. 문은 열거나 닫을 수 있듯이, 게이트는 데이터의 흐름을 제어한다.  
LSTM에서 사용하는 게이트는 '열기/닫기' 뿐 아니라, 어느 정도 열지를 조절할 수 있다. '어느 정도'를   
'열림 상태'라 부르며 0.7(70%)이나 0.2(20%)처럼 제어할 수 있다.

![3](/assets/images/LSTM/3.PNG)

게이트의 열림 상태는 그림에서처럼 0.0~1.0 사이의 실수로 나타나며 1.0은 완전한 개방,  
그리고 그 값이 다음으로 흐르는 물의 양을 결정한다. 여기서 중요한 것은 '게이트를 얼마나 열까'  
라는 것도 데이터로부터 자동으로 학습한다는 점이다.

### 6.1.3 tanh, 시그모이드 함수의 역할

- tanh:  
tanh의 출력은 -1.0 ~ 1.0의 실수이다. -1.0 ~ 1.0의 수치를 그 안에 인코딩된 '정보'의 강약(정도)를
해석할 수 있다.
- 시그모이드 :  
시그모이드 함수의 출력은 0.0 ~ 1.0의 실수이며, 데이터를 얼마만큼 통과시킬지를 정하는 비율을 뜻한다.

따라서 게이트에서는 시그모이드 함수가, 실질적인 정보를 지니는 데이터에는 tanh 함수가 활성화 함수로 사용된다.

6.2 LSTM 구조
---

### 6.2.1 Forget 게이트

Forget 게이트는 과거 정보를 버릴지 말지 결정하는 과정이다.

![4](/assets/images/LSTM/4.PNG)

과거의 정보를 통해 맥락을 고려하는 것도 중요하지만, 그 정보가 필요하지 않을 경우에는 과감히  
버리는 것도 중요하다.

- 활성화 함수로 시그모이드(sigmoid)를 사용하므로, 0.0 ~ 1.0 값이 나온다.
  - '0'일 경우, 이전의 cell state 값은 모두 '0'이 되어 과거의 정보를 잊는다는 것이고
  - '1'일 경우, 과거의 정보를 100% 가져와 미래의 예측 결과에 영향을 준다는 것이다.
- 즉, Forget 게이트는 현재 입력과 이전 출력을 고려해서, cell state의 과거기억을 얼마나 갖고올지를 출력한다.

### 6.2.2 Input 게이트

Input 게이트는 현재정보를 저장할지 결정하는 과정이다.

![5](/assets/images/LSTM/5.PNG)

과거의 정보를 반영했으니, 현재의 정보를 반영해야 한다.  

- 현재의 정보를 계산하는 두 가지의 식이 있다.
- it는 활성화 함수로 시그모이드(sigmoid)를 사용한다. 
- Ct는 활성화 함수로 tanh를 사용한다.
- it * Ct를 함으로써 현재의 정보를 반영한다.

**Forget 게이트와 Input 게이트의 주요 역할**

이전 cell 게이트 값을 얼마나 반영할지, 지금 입력과 이전 출력으로 얻어진 값을 얼마나  
cell 게이트에 반영할지 정하는 역할

### 6.2.3 cell state

cell state는 이전 cell의 정보를 현재 cell의 정보로 업데이트를 하는 과정이다.

![6](/assets/images/LSTM/6.PNG)

- Forget 게이트를 통해서 과거정보를, Input 게이트를 통해서 현재정보를 정했으니,  
Input gate * current state + Forget gate * previous state를 하며 업데이트를 해준다.

### 6.2.4 Output 게이트

Output 게이트는 어떤 출력값을 출력할지 결정하는 과정이다.

![7](/assets/images/LSTM/7.PNG)

최종적으로 얻어진 cell state 값(tanh 값)을 얼마나 반영할지 결정하는 역할 

- cell state에서 업데이트된 Ct를 tanh함수로 활성화 하여 실질적인 정보(-1.0~1.0)을 만들고,  
시그모이드 함수를 사용해 나온 게이트값과 곱해 출력을 한다.

6.3 LSTM 구현
---

### 6.3.1 Time LSTM 구현

![8](/assets/images/LSTM/8.PNG)

Time LSTM은 T개분의 시계열 데이터를 한꺼번에 처리하는 계층이다.

### 6.3.2 RNNLM 추가 개선

RNNLM의 개선 포인트 3가지를 설명하고 구현하고 얼마나 좋아졌는지 본다.

1. LSTM 계층 다층화

![9](/assets/images/LSTM/9.PNG)

RNNLM으로 정확한 모델을 만들고자 한다면 많은 경우 LSTM 계층을 깊게 쌓하 효과를 볼 수 있다.  
지금까지는 1층만 사용했지만 2층, 3층 식으로 여러 겹 쌓으면 언어 모델의 정확도가 향상될 수 있다.  

2. 드롭아웃에 의한 과적합 억제

LSTM 계층을 다층화하면 시계열 데이터의 복잡한 의존 관계를 학습할 수 있을 것이라 기대할 수 있다.  
하지만 층을 깊게 쌓음으로써 표현력이 풍부한 모델을 만들 수 있으나 종종 **과적합**을 일으킨다.  
불행하게도 RNN은 일반적인 피드포워드 신경망보다 쉽게 과적합을 일으킨다는 소식이다.

![10](/assets/images/LSTM/10.PNG)

드롭아웃은 무작위로 뉴런을 선택해 선택한 뉴런을 무시한다. 무시한다는 말은 그 앞  
계층으로부터의 신호 전달을 막는다는 뜻이다. 이 '무작위한 무시'가 제약이 되어 신경망의 일반화 성능을 개선한다.

![11](/assets/images/LSTM/11.PNG)

위 그림은 드롭아웃 계층을 활성화 함수 뒤에 삽입하는 방법으로 과적합 억제에 기여하는 모습이다.

![12](/assets/images/LSTM/12.PNG)

다음과 같이 드롭아웃 계층을 깊이 방향으로 삽입하면 시간이 흐름에 따라 정보가 사라지지 않고, 노이즈도 축적되지 않는다.

3. 가중치 공유

![13](/assets/images/LSTM/13.PNG)

가중치 공유는 Embedding 계층의 가중치와 Affine 계층의 가중치를 연결하는 기법이다.  
두 계층이 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도도  
향상되는 일석이조의 기술이다.


6.4 정리
---

- 단순한 RNN의 학습에서는 기울기 소실이 일어나는데 이를 개선한 모델이 LSTM이다.
- LSTM에는 Input, Forget, Output 게이트가 있다.
- 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용하여 0.0 ~ 1.0 사이의 실수를 출력한다.
- 언어 모델 개선에는 LSTM 계층 다층화, 드롭아웃, 가중치 공유 등의 기법이 효과적이다.
