# word2vec study

---
layout: single
title:  "word2vec 공부"
---


**이 글은 밑바닥부터 시작하는 딥러닝 2 책을 참고로 작성**

[밑바닥부터 시작하는 딥러닝 2](https://github.com/WegraLee/deep-learning-from-scratch-2)

1.1 추론 기반 기법과 신경망
---

단어를 벡터로 표현하는 방법은 크게 두 부분이 있다.

1. 통계 기반 기법

2. 추론 기반 기법

두 기법의 배경에는 모두 분포 가설이 있다.


### 1.1.1 통계 기반 기법의 문제점

통계 기반 기법의 대표적인 모델은 SVD이다.

말뭉치 안의 각 단어에 대해서 예측할 단어의 주변 단어의 빈도를 집계(동시발생 행렬) 해서

동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 "희소벡터"를 작은

"밀집벡터"로 변환해 분산 표현을 얻는 방식이다.

하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.

SVD를 n*n 렬에 적용하는 비용은 O(n3) 이다.

한편, 추론 기반 기법에서는 미니배치로 학습하는 것이 일반적이다.

![1](/assets/images/word2vec/1.PNG)

통계 기반 기법은 학습 데이터를 한꺼번에 처리한다.

반면, 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습한다.


### 1.1.2 추론 기반 기법 개요

추론 기반 기법에서는 추론이 주된 작업이다.

추론이란, 주변 단어(맥락)이 주어졌을 때, ? 에 무슨 단어가 들어가는지를 추측하는 작업이다.

![2](/assets/images/word2vec/2.PNG)

추론 문제를 풀고 학습하는 것이 추론 기반 기법이 다루는 문제이다.

이러한 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습하는 것이다.

즉, 예측할 단어의 주변 단어(맥락)을 가지고 심층신경망을 통해 나온 확률 분포로 추론한다.

![3](/assets/images/word2vec/3.PNG)


### 1.1.3 신경망에서의 단어 처리

심경망에서 학습을 하려면 단어들을 벡터로 바꿔야 한다. 

이때 원핫 표현을 통해 단어를 벡터로 변환한다.

![4](/assets/images/word2vec/4.PNG)

단어를 벡터로 나타낼 수 있고, 신경망을 구성하는 계층들은 벡터를 처리할 수 있다.

**즉, 단어를 신경망으로 처리할 수 있다.**

원 핫표현으로 된 단어 하나를 완전연결계층을 통해 변환하는 모습

![5](/assets/images/word2vec/5.PNG)

맥락 c와 가중치 W의 곱으로 해당 위치의 행벡터가 추출된다.

![6](/assets/images/word2vec/6.PNG)

W의 열의 개수가 많으면 가중치를 계산하는 과정에서 속도가 느려진다.

따라서 최대한 작게 만들어 압축하는 것이 좋다.


1.2 단순한 word2vec
---

지금 부터 할 일은 모델을 신경망으로 구축하는 것이다.

word2vec에서 제안하는 CBOW, continuous bag-of-words모델을 구현한다.


### 1.2.1 CBOW 모델의 추론 처리

CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다.

타깃은 중앙 단어이고, 그 주변 단어들이 맥락이다.

우리는 이 CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낼 것이다.

![7](/assets/images/word2vec/7.PNG)

위 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문이다.

은닉층을 보면 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환되는 데, 전체를 평균한 값이 은닉층으로 간다.

출력층은 각 단어들의 score값으로 각 Softmax 계층에 통과하면 가장 높은 값이 해당 단어를 출력할 확률도 높아진다. 

#### 이제 CBOW 모델을 계층 관점에서 본다.

![8](/assets/images/word2vec/8.PNG)

다음과 같이 예측할 단어(say)의 맥락(그림에서는 you, goodbye)을 원핫벡터로 나타내고, 

가중치 행렬(7x3)에 곱한 후 더해준다. 또한 단어의 개수만큼 나눠준 후 다시한번 가중치를 곱해 score를 도출한다.


### 1.2.2 CBOW 모델의 학습

지금까지 설명한 CBOW 모델은 출력층에서 각 단어의 score값을 출력했다.

이제 score 값을 softmax 함수에 적용하면 확률을 얻을 수 있다.

![9](/assets/images/word2vec/9.PNG)

CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정한다.

그 결과, 입/출력측 가중치에 단어의 출현 패턴을 파악한 벡터가 학습된다.

#### 이제 도출된 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후 
#### 그 값을 손실로 사용해 학습을 진행한다.

![10](/assets/images/word2vec/10.PNG)

추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy 계층을 추가한 

것만으로도 손실을 얻을 수 있다.


### 1.2.3 word2vec 의 가중치와 분산 표현

word2vec 에서 사용되는 신경망에는 두 가지 가중치가 있다.

입력 측 가중치와 출력 측 가중치이다.

그리고 입력 측 가중치의 각 행이 각 단어의 분산 표현에 해당한다.

다만, 출력 측 가중치는 각 단어의 분산 표현이 열 방향으로 저장된다.

![11](/assets/images/word2vec/11.PNG)

####  최종적으로 이용하는 단어의 분산 표현으로는 어느 쪽 가중치를 사용하면 좋을까?

1. 입력 측의 가중치만 이용

2. 출력 측의 가중치만 이용

3. 양쪽 가중치를 모두 이용

**word2vec, 특히 skip-gram 모델에서는 입력 측 가중치만 이용하는 것이 가장 대중적이다.**


1.3 학습 데이터 준비
---

### 1.3.1 맥락과 타깃

word2vec에서 이용하는 신경망의 입력은 맥락이다.

그리고 정답 레이블은 맥락에 둘러싸인 중앙의 단어, 즉 타깃이다.

우리가 해야 할 일은 신경망에 맥락을 입력했을 때 타깃이 출현할 확률을 높이는 것이다.

예를 들면,

![12](/assets/images/word2vec/12.PNG)

말뭉치로부터 맥락과 타깃을 만드는 함수를 구현하기 전에,

말뭉치 텍스트를 단어 ID 로 변환해야 한다.

```python
import sys
sys.path.append('..')
from common.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
print(corpus)
# [0 1 2 3 4 1 5 6

print(id_to_word)
# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'} 
```

단어 ID의 배열로부터 맥락과 타깃을 작성하는 예 (맥락의 윈도우 크기는 1)

![13](/assets/images/word2vec/13.PNG)

맥락과 타깃을 만드는 함수를 구현하면

```python
def create_co_matrix(corpus, vocab_size, window_size=1):
    '''동시발생 행렬 생성
    :param corpus: 말뭉치(단어 ID 목록)
    :param vocab_size: 어휘 수
    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)
    :return: 동시발생 행렬
    '''
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
            left_idx = idx - i
            right_idx = idx + i

            if left_idx >= 0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] += 1

            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1

    return co_matrix
```
다음과 같이 구현했다. 이제 맥락과 타깃의 원소를 원핫 표현으로 변환 한 후

CBOW 모델에 넘겨주면 된다.


### 1.3.2 원핫 표현으로 변환

지금 까지 말뭉치들을 CBOW 모델에 학습하기 위한 벡터로 변환 해주었다.

![14](/assets/images/word2vec/14.PNG)

그림과 같이 말뭉치들을 맥락(입력)과 타깃(출력)으로 나눠준 후

단어 ID를 통해 숫자로 변환 해주고, 

원핫 표현으로 0과 1의 표현으로 학습할 준비가 끝났다.

1.4 CBOW 모델 구현
---

CBOW 모델을 구현 해보자.

![15](/assets/images/word2vec/15.PNG)

```python
import sys
sys.path.append('..')
import numpy as np
from common.layers import MatMul, SoftmaxWithLoss
```

```python
class SimpleCBOW:
    def __init__(self, vocab_size, hidden_size):
        V, H = vocab_size, hidden_size

        # 가중치 초기화
        W_in = 0.01 * np.random.randn(V, H).astype('f')
        W_out = 0.01 * np.random.randn(H, V).astype('f')

        # 계층 생성
        self.in_layer0 = MatMul(W_in)
        self.in_layer1 = MatMul(W_in)
        self.out_layer = MatMul(W_out)
        self.loss_layer = SoftmaxWithLoss()

        # 모든 가중치와 기울기를 리스트에 모은다.
        layers = [self.in_layer0, self.in_layer1, self.out_layer]
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

        # 인스턴스 변수에 단어의 분산 표현을 저장한다.
        self.word_vecs = W_in
```

신경망의 순전파인 forward() 메서드를 구현한다.

```python
    def forward(self, contexts, target):
        h0 = self.in_layer0.forward(contexts[:, 0])
        h1 = self.in_layer1.forward(contexts[:, 1])
        h = (h0 + h1) * 0.5
        score = self.out_layer.forward(h)
        loss = self.loss_layer.forward(score, target)
        return loss
```

마지막으로 역전파인 backward()를 구현한다.

```python
    def backward(self, dout=1):
        ds = self.loss_layer.backward(dout)
        da = self.out_layer.backward(ds)
        da *= 0.5
        self.in_layer1.backward(da)
        self.in_layer0.backward(da)
        return None
```

CBOW 모델의 역전파

![16](/assets/images/word2vec/16.PNG)

### 1.4.1 학습 코드 구현

CBOW 모델의 학습은 일반적인 신경망의 학습과 완전히 같다.

맥락과 타깃이 각각 입력값과 출력값이 되고, 학습 데이터를 신경망에 입력한 다음,

모델에 나온 스코어값(score)에 softmax 함수를 이용해 확률 표현을 만든 다음,

실제값과 예측값을 비교 및 계산을 한다.

마지막으로 Loss(기울기)를 구하고 가중치 매개변수를 순서대로 갱신해간다.

```python
import sys
sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
from common.trainer import Trainer
from common.optimizer import Adam
from simple_cbow import SimpleCBOW
from common.util import preprocess, create_contexts_target, convert_one_hot


window_size = 1
hidden_size = 5
batch_size = 3
max_epoch = 1000

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)
contexts, target = create_contexts_target(corpus, window_size)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)

model = SimpleCBOW(vocab_size, hidden_size)
optimizer = Adam()
trainer = Trainer(model, optimizer)

trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()
```

매개변수 갱신 방법은 Adam으로 한다.

코드 결과

![17](/assets/images/word2vec/17.PNG)

다음과 같은 함수가 출력 됐다. 

학습을 거듭할수록 손실이 줄어드는 것을 확인할 수 있다.

1.5 word2vec 보충
---

지금까지 word2vec의 CBOW 모델을 살펴봤다.

이제 CBOW 모델을 확률 관점에서 살펴보자.


### 1.5.2 CBOW 모델과 확률

확률의 표기법을 간단하게 살펴보자.

확률 P()

동시 확률 P(A,B), A와 B가 동시에 일어날 확률.

사후 확률 P(AㅣB), 사건이 일어난 후의 확률.

B라는 정보가 주어졌을 때, A가 일어날 확률.<br />

그럼 CBOW 모델을 확률 표기법으로 기술해보자.

CBOW 모델이 하는 일은, 맥락을 주면 타깃 단어가 출현할 확률을 출력하는 것이다.

![18](/assets/images/word2vec/18.PNG)

맥락의 단어로부터 타깃 단어를 추측하는 것이다.

CBOW 모델은 다음 식을 모델링 하고 있다.

![19](/assets/images/word2vec/19.PNG)

위의 식을 이용하면 CBOW 모델의 손실 함수도 간결하게 표현할 수 있다.
 
교차 엔트로피 오차에 적용하여 나타내면, 
 
![20](/assets/images/word2vec/20.PNG)


### 1.5.3 skip-gram 모델

word2vec은 2개의 모델을 제안하고 있다.
1. CBOW 모델
2. skip-gram 모델<br />

skip-gram은 CBOW 에서 다루는 맥락과 타깃을 역전시킨 모델이다.

![21](/assets/images/word2vec/21.PNG)

다음과 같이 skip-gram 모델은 중앙의 단어(타깃)으로 부터 맥락을 추측한다.

![22](/assets/images/word2vec/22.PNG)

skip-gram 모델을 확률 표기로 나타내보면,

![23](/assets/images/word2vec/23.PNG)

위 식을 교차 엔트로피 오차에 적용하여 skip-gram 모델의 손실 함수를 유도할 수 있다.

![24](/assets/images/word2vec/24.PNG)

<br />

그러면 CBOW 모델과 skip-gram 모델 중 어느 것을 사용해야 할까?

정답은 skip-gram 모델이다.

단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많기 때문이다.

특히 말뭉치가 커질수록 유추 문제의 성능 면에서 skip-gram 모델이 더 뛰어나다.

1.6 정리
---

- 단어를 벡터로 표현하는 기법 중 통계 기반 기법은 비용문제로 추론 기반 기법을 사용한다.
- 단어를 벡터로 표현함으로써 심층신경망으로 처리할 수 있다. (word2vec)
- word2vec 중 CBOW모델은 맥락으로부터 타깃을 추측하는 신경망이다.
- CBOW 모델의 순전파는 두 단어의 벡터를 가중치 행렬에 곱하고, 더하고, 나누고,
다시 가중치 행렬에 곱하면 score값이 나오고, 정답 레이블과 softmax with Loss 를 통해 LOSS를 구한다.
- CBOW 모델의 역전파는 LOSS의 기울기(변화량)을 구해 가중치를 갱신한다.
- skip-gram 모델은 타깃(중앙의 단어)로부터 맥락을 추측한다.
- CBOW, skip-gram 중 skip-gram이 성능이 뛰어나다.

<br />

----------------------------------------

앞서 보았던 CBOW 모델은 처리 효율이 떨어져 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커진다.

<br />

따라서 단순한 word2vec에 두 가지 개선을 추가한다.

1. **Embedding** 이라는 새로운 계층을 만든다.
2. **네거티브 샘플링** 이라는 새로운 손실함수를 도입한다


이로써 개선된 word2vec을 완성할 수 있다.

----------------------------------

만약 어휘가 100만개, 은닉층의 뉴런이 100개인 CBOW 모델을 생각해 보면,

![25](/assets/images/word2vec/25.PNG)

입력층과 출력층에 각 100만개의 뉴런이 존재하고, 다음 두 계산이 병목된다.

1. 입력층의 원핫 표현과 가중치 행렬 Win 의 곱 계산 (2.1 절에서 해결)
2. 은닉층과 가중치 행렬 Wout의 곱 및 소프트맥스 계층의 계산 (2.2 절에서 해결)

2.1 word2vec의 개선 1
---

첫 번째 문제는 단어를 원핫으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기도 커진다.

즉, 상당한 메모리를 차지하며 가중치 행렬을 곱할 때 계산 자원이 매우 낭비된다.

이러한 문제는 **Embedding 계층**을 도입하는 것으로 해결한다.

두 번째 문제는 은닉층 이후의 계산이다. Softmax 계층에서도 다루는 어휘가 많아짐에 따라 계산량이 증가한다.

이러한 문제는 네거티브 샘플링이라는 새로운 손실 함수를 도입해 해결한다.

### 2.1.1 Embedding 계층

Embedding 이란, 텍스트를 구성하는 하나의 단어를 수치화하는 방법의 일종이다.

앞서 임베딩 계층이 필요한 이유에 대해 알아봤다.

즉, 수많은 입력벡터와 가중치 행렬을 곱하면 메모리가 상당히 차지하기 때문이다.

하지만 결과적으로 수행하는 일은 다음과 같다.

![6_2](/assets/images/word2vec/6.PNG)

단지 행렬의 특정 행을 추출하는 것뿐이다. 

**따라서 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없다.** 

특정 행을 추출하는 것은 index_num으로 접근하면 편하기 때문이다.

```python
idx = np.array([0])
W[idx] # h
```
즉, 기존의 원핫벡터에서 다음과 같은 실수 벡터로 바꿔주는 계층을 임베디드 계층이라고 하며,

필요한 이유이다.

![26](/assets/images/word2vec/26.PNG)

임베딩 계층의 순전파는 가중치 W의 특정 행을 추출하는 것이다. 

반대로 역전파는 앞 층(출력층) 으로부터 전해진 기울기를 다음 층으로 그대로 흘려주면 된다.

다만, 앞 층으로부터 전해진 기울기를 가중치 기울기 dW의 특정 행(idx 번째 행)에 설정한다.

![27](/assets/images/word2vec/27.PNG)

2.2 word2vec의 개선 2
---

남은 문제점은   
1. 은닉층의 뉴런과 가중치 행렬의 곱  
2. 소프트맥스 계층의 계산  
이다.

### 2.2.1 네거티브 샘플링

**네거티브 샘플링**의 핵심은 **다중분류**를 **이진분류**로 근사하는 것이다.

예를 들면, 다중 분류는 맥락이 you와 goodbye일때, 타깃 단어는 무엇일까?에 대해 대답하는 것이고,

이진 분류는 맥락이 you와 goodbye일때, 타깃 단어는 say일까?에 대해 대답하는 것이다.

이런식으로 하면 출력층에 뉴런을 하나만 준비하면 된다. 

즉, 출력층에 나온 점수(score)값을 softmax 함수로 이용해 100만번 계산한 후 LOSS를 구하지 않고

score벡터에서 "say"에 해당하는 열벡터만을 한번 계산(Sigmoid)해서 확률을 구한다.

![28](/assets/images/word2vec/28.PNG)

따라서, 은닉층과 출력층의 가중치 행렬의 내적은 say에 해당하는 열만 추출하고, 추출된

벡터와 은닉층 뉴런과의 내적을 구하면 끝이다.

### 2.2.2 시그모이드 함수와 교차 엔트로피 오차

이진 분류 문제를 신경망으로 풀려면 score값에 시그모이드 함수를 적용해 확률로 변환하고,

손실을 구할 때는 손실 함수로 '교차 엔트로피 오차'를 사용한다.

시그모이드 함수를 적용해 확률 y를 얻으면, 이 확률 y로부터 손실을 구한다. 

![29](/assets/images/word2vec/29.PNG)

여기서 t는 정답 레이블이다. 이 정답 레이블의 값은 0 혹은 1 이다.(이진 분류)

따라서 t가 1이면 L = -logy가 출력되고, t가 0이면 -log(1-y)가 출력된다.

![31](/assets/images/word2vec/31.PNG)

위의 식은 score의 변화에 따른 LOSS를 구한 것이다.

### 2.2.3 다중분류에서 이진 분류로 (구현)

다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는 데 중요한 개념이다.

즉 target 단어에 해당하는 index의 값만 확률로 구하는 것이 목표

![30](/assets/images/word2vec/30.PNG)

### 2.2.4 네거티브 샘플링

2.2.3절 까지는  target, 즉 정답인 단어에 해당하는 Loss만 구하게 된다.

그렇다면 정답이 아닌 다른 단어에 대한 확률값은 어떻게 구할지 잘 학습하지 못한다.

정답이 아닌 단어는 낮은 확률을 예측할 수 있게 학습하도록 부정적인 예, negative sample 몇 가지를 더 넣어야한다.

즉, target = [0,1,0,...,0] 에서 정답 index인 1을 예측하는 것에서 나머지 레이블을 0으로 예측하는 것이다.

하지만 모든 부정적인 예를 대상으로 하면 어휘 수가 늘어나기 때문에 비효율적이다.

그렇기 때문에 부정적 예를 선택해서 sampling 해야한다. 이것이 **네거티브 샘플링** 기법이다.

### 2.2.5 네거티브 샘플링의 샘플링 기법

그렇다면 부정적인 예를 어떻게 선택해야 할까?

답은 말뭉치의 단어별 출현 횟수를 바탕으로 확률 분포를 구한다.

이때, 출현 빈도가 낮은 단어의 선택을 높여주기 위해 확률 분포에서 구한 값들 0.75 제곱하고 해당 확률 값을 다시 구한다.

즉, 출현 빈도가 낮은 단어의 확률 값을 높여주고, 다른 확률 값은 상대적으로 낮출 수 있게 되어 비교적 골고루 단어가 선택되도록 하는 것이 목적이다.


2.3 개선판 word2vec 학습
---

![32](/assets/images/word2vec/32.PNG)

다음과 같이 출력층에서 나온 score와 target이 Sigmoid with Loss로 바로 입력 되는 것이 아니라,

네거티브 샘플링과 같이 입력되어 LOSS를 구한다.


2.4 정리
---

- Embedding 계층은 단어의 분산 표현을 담고 있다.
- word2vec의 개선을 위해 다음 2가지 작업을 수행했다.
  - Embedding 계층에서 특정 단어의 index만 뽑아 계산
  - 부정적 샘플링을 통해 다중 분류를 이진 분류로, 몇 가지의 단어들의 확률값과 Loss를 계산하도록
- 마지막으로 지금 까지 배운 word2vec의 개선된 CBOW 도식화이다.

![33](/assets/images/word2vec/33.PNG)
